{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py:1078: UserWarning: Illegal line #210\n",
      "\t\"\t\t\t Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, \n",
      "\"\n",
      "\tin file \"/usr/local/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\"\n",
      "  warnings.warn('Illegal %s' % error_details)\n",
      "/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py:1078: UserWarning: Illegal line #211\n",
      "\t\"\t\t\tsans-serif\n",
      "\"\n",
      "\tin file \"/usr/local/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\"\n",
      "  warnings.warn('Illegal %s' % error_details)\n"
     ]
    }
   ],
   "source": [
    "from feature_engineer import *\n",
    "import pandas as pd\n",
    "import time\n",
    "from sqlalchemy.engine import create_engine\n",
    "engine_1 = create_engine(\n",
    "        'mysql+pymysql://ro:cKqj4E3$K7GGeqs@nshd-slave-bi.mysql.rds.aliyuncs.com/paydayloan?charset=utf8',\n",
    "        echo=False,pool_size=20, max_overflow=0)\n",
    "engine_2 = create_engine(\n",
    "    'mysql+pymysql://riskcontrol:tuLwJ3G6FLwR6t4A@nshd-risk.mysql.rds.aliyuncs.com/riskcontrol?charset=utf8',\n",
    "    echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_accept = pd.read_csv(r'/home/baowu/code/model/synthesis/feature_bw_all.csv')\n",
    "\n",
    "sql = \"\"\"select uid from ydata where startDate >= '2017-6-7' and startDate <= '2017-9-25' and loan_time = 1;\"\"\"\n",
    "users_df = pd.read_sql(sql, engine_2)\n",
    "features_part = pd.merge(users_df, features_accept, how='left', on='uid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConVar = ['repayState_9_91', 'clock_number', 'place_mode_rate', 'loanPeriod_1_91', 'loanAmount_mode_rate_91', 'comm_gap_std', 'express_numbers_count_all', 'zmxyScore', 'fee_mode_rate', 'registerGap_median_hd', 'loanPeriod_max_91', 'repayState_0_91', '客户行为检测_3个月内身份证关联多个申请信息_3个月身份证关联邮箱数', 'num_coll_call', 'age', 'loanAmount_4_91', 'zk_v_3_4', 'clock_mode_rate', 'loangap_max_91']\n",
    "CatVar = ['career_企业高中级主管', 'loanPeriod_21_91', '客户行为检测_3个月内身份证关联多个申请信息_risk_level_low', 'workExp_10年以上', 'gender_male', 'industry_制造业', '多平台借贷申请检测_18个月内申请人在多个平台申请借款_互联网金融门户', 'career_企业负责人、股东', 'marr_single', 'workExp_5-7年', 'one_hot_V3_V_BC_CN_UK', 'one_hot_V2_V_PH_CN_MA_UM180D', '多平台借贷申请检测_1个月内申请人在多个平台申请借款_大型消费金融公司', 'has_over_hd', 'CHINA_TELECOM', 'has_arrears_91', 'rOs_Android', 'workExp_', 'repayS_9_91', 'loanA_0_91', 'black_list_91', 'workExp_1年以下', '不良信息扫描_手机号命中中风险关注名单_fraud_type_异常借款', 'device_info_deviceType_iPhone', 'loanA_-6_91', 'one_hot_F1_not_match', 'houseProp_无房', '多平台借贷申请检测_1个月内申请人在多个平台申请借款_第三方服务商', 'rFrom_ANDROID_APP', 'houseProp_有房有贷款', 'houseProp_', 'key_3', 'rOs_iOS', 'workExp_2年', 'degree_bachelor', 'one_hot_V3_V_BC_CN_MA_UL180D', 'has_od_91', '不良信息扫描_身份证命中中风险关注名单_fraud_type_信用异常', 'income_100000-200000', 'career_个体商店老板', 'loanPeriod_9_91', 'province_天津市', 'loanT_1_91']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    IV_path = '/home/heyang/data/IVAll/result_part/'\n",
    "    features_con_woe = [list(features_part.uid),list(features_part.label)]\n",
    "    for var in ConVar:\n",
    "        data = pd.read_csv(IV_path + var + '.csv')\n",
    "        Cutpoint = list(data.Cutpoint)\n",
    "        Cutpoint = Cutpoint[0:len(Cutpoint) - 2]\n",
    "        Cutpoint = list(set([float(x.split(' ')[1]) for x in Cutpoint]))\n",
    "        Cutpoint = [-100000] + Cutpoint\n",
    "        Cutpoint.append(1000000)\n",
    "        Cutpoint = sorted(Cutpoint)\n",
    "        WoE = list(data.WoE)\n",
    "        varvalues_new = []\n",
    "        varvalues_old = list(features_part[var])\n",
    "        for val in varvalues_old:\n",
    "            if val == val:\n",
    "                for i in range(len(Cutpoint) - 1):\n",
    "                    if val > Cutpoint[i] and val <= Cutpoint[i + 1]:\n",
    "                        varvalues_new.append(WoE[i])\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                varvalues_new.append(0)\n",
    "        features_con_woe.append(varvalues_new)\n",
    "    features_con_woe = pd.DataFrame(features_con_woe).T\n",
    "    features_con_woe.columns = ['uid','label'] + ConVar\n",
    "\n",
    "    features_select = pd.merge(features_con_woe, features_part[['uid'] + CatVar], how='left', on='uid')\n",
    "    features_select = features_select.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet = features_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#测试集\n",
    "sql = \"\"\"select uid from ydata where startDate < '2017-6-7' and loan_time = 1;\"\"\"\n",
    "UidTest = pd.read_sql(sql,engine_2)\n",
    "TestSet = pd.merge(UidTest,features_accept,how='left',on='uid').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    IV_path = '/home/heyang/data/IVAll/result_part/'\n",
    "    features_con_woe = [list(TestSet.uid),list(TestSet.label)]\n",
    "    for var in ConVar:\n",
    "        data = pd.read_csv(IV_path + var + '.csv')\n",
    "        Cutpoint = list(data.Cutpoint)\n",
    "        Cutpoint = Cutpoint[0:len(Cutpoint) - 2]\n",
    "        Cutpoint = list(set([float(x.split(' ')[1]) for x in Cutpoint]))\n",
    "        Cutpoint = [-100000] + Cutpoint\n",
    "        Cutpoint.append(1000000)\n",
    "        Cutpoint = sorted(Cutpoint)\n",
    "        WoE = list(data.WoE)\n",
    "        varvalues_new = []\n",
    "        varvalues_old = list(TestSet[var])\n",
    "        for val in varvalues_old:\n",
    "            if val == val:\n",
    "                for i in range(len(Cutpoint) - 1):\n",
    "                    if val > Cutpoint[i] and val <= Cutpoint[i + 1]:\n",
    "                        varvalues_new.append(WoE[i])\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                varvalues_new.append(0)\n",
    "        features_con_woe.append(varvalues_new)\n",
    "    features_con_woe = pd.DataFrame(features_con_woe).T\n",
    "    features_con_woe.columns = ['uid','label'] + ConVar\n",
    "\n",
    "    features_select = pd.merge(features_con_woe, TestSet[['uid'] + CatVar], how='left', on='uid')\n",
    "    features_select = features_select.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TestSet = features_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time: 0.00 min 0.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "start = time.clock()\n",
    "var_name = ConVar + CatVar\n",
    "X_train = TrainSet[var_name]\n",
    "X_test = TestSet[var_name]\n",
    "y_train = TrainSet['label']\n",
    "y_test = TestSet['label']\n",
    "end = time.clock()\n",
    "print(\"run time: %.2f min %.2f s\" % divmod((end - start), 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#xgboost\n",
    "def modelfit(alg,trainset,train_target,testset,test_target,users,useTrainCV=True,cv_folds=5,early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(trainset.values, label=train_target.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds,)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(trainset,train_target,eval_metric='auc')\n",
    "    #feature_importance = sorted(alg.feature_importances_,reverse=True)\n",
    "    \n",
    "#     var_name = list(trainset.columns)\n",
    "#     top10 = feature_importance[0:10]\n",
    "#     top10_var = [var_name[feature_importance.index(impor)] for impor in top10]\n",
    "#     print(\"\\n\".join(top10_var))\n",
    "#     print(top10)\n",
    "    \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(trainset)\n",
    "    dtrain_predprob = alg.predict_proba(trainset)[:,1]\n",
    "    \n",
    "    #Predict testing set:\n",
    "    dtest_predictions = alg.predict(testset)\n",
    "    dtest_predprob = alg.predict_proba(testset)[:,1]\n",
    "    pre = pd.DataFrame([users,list(dtest_predictions),list(dtest_predprob)]).T\n",
    "    pre.columns = ['userId','dtest_predictions','dtest_predprob']\n",
    "    \n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"Model Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(train_target.values, dtrain_predictions))\n",
    "    print(\"precision (Train): %.4g\" % metrics.precision_score(train_target.values, dtrain_predictions))\n",
    "    print(\"recall (Train): %.4g\" % metrics.recall_score(train_target.values, dtrain_predictions))\n",
    "    print(\"F1_score (Train): %.4g\" % metrics.f1_score(train_target.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_target.values, dtrain_predprob))\n",
    "    print(\"confusion_matrix (Train):\")\n",
    "    print(metrics.confusion_matrix(train_target.values, dtrain_predictions))\n",
    "    \n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(test_target.values, dtest_predictions))\n",
    "    print(\"precision (Test): %.4g\" % metrics.precision_score(test_target.values, dtest_predictions))\n",
    "    print(\"recall (Test): %.4g\" % metrics.recall_score(test_target.values, dtest_predictions))\n",
    "    print(\"F1_score (Test): %.4g\" % metrics.f1_score(test_target.values, dtest_predictions))\n",
    "    print(\"AUC Score (Test): %f\" % metrics.roc_auc_score(test_target.values, dtest_predprob))\n",
    "    print(\"confusion_matrix (Test):\")\n",
    "    print(metrics.confusion_matrix(test_target.values, dtest_predictions))\n",
    "    \n",
    "    \n",
    "    fpr_lr, tpr_lr, thresholds_lr = metrics.roc_curve(test_target, dtest_predprob)\n",
    "    print('lr ks:',abs(fpr_lr - tpr_lr).max(),'lr AUC:', metrics.auc(fpr_lr, tpr_lr))\n",
    "    \n",
    "    feature_importances_df=pd.DataFrame({'features':trainset.columns,'score':alg.feature_importances_})\n",
    "    feature_importances_df.sort_values('score', ascending = False).head(10)\n",
    "    return(feature_importances_df,pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Report\n",
      "Accuracy : 0.8227\n",
      "precision (Train): 0.7063\n",
      "recall (Train): 0.239\n",
      "F1_score (Train): 0.3571\n",
      "AUC Score (Train): 0.795104\n",
      "confusion_matrix (Train):\n",
      "[[24337   644]\n",
      " [ 4933  1549]]\n",
      "Accuracy : 0.7268\n",
      "precision (Test): 0.6251\n",
      "recall (Test): 0.4334\n",
      "F1_score (Test): 0.5119\n",
      "AUC Score (Test): 0.744984\n",
      "confusion_matrix (Test):\n",
      "[[10794  1589]\n",
      " [ 3463  2649]]\n",
      "lr ks: 0.35738301074 lr AUC: 0.744983985973\n",
      "run time: 49.00 min 11.40 s\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "start = time.clock()\n",
    "xgb1 = XGBClassifier(learning_rate =0.1,n_estimators=500,max_depth=4,min_child_weight=1,gamma=0,subsample=0.8,\n",
    "                     colsample_bytree=0.8,objective= 'binary:logistic',nthread=-1,scale_pos_weight=1,seed=27)\n",
    "features_importance,pre_result = modelfit(xgb1,X_train,y_train,X_test,y_test,list(TestSet.uid))\n",
    "end = time.clock()\n",
    "print(\"run time: %.2f min %.2f s\" % divmod((end - start), 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
